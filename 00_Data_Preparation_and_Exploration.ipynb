{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cdc4493-184f-4b4a-af93-b5dc57d891fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas com sucesso.\n",
      "Caminho base do DROZY definido para: DROZY\n",
      "Shape predictor de dlib carregado.\n",
      "Detector de face de dlib (frontal) carregado.\n"
     ]
    }
   ],
   "source": [
    "# Célula 1: Configuração Inicial e Importações\n",
    "\n",
    "# Importação de bibliotecas essenciais\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2 # OpenCV para leitura de vídeo e processamento de imagens\n",
    "import dlib # Para detecção de face e 68 landmarks faciais\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import dump # Para salvar dados processados\n",
    "\n",
    "# Configurações de exibição do Pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Desativar avisos (útil para notebooks de desenvolvimento)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso.\")\n",
    "\n",
    "# --- Definir caminhos para o dataset DROZY ---\n",
    "# IMPORTANTE: Ajuste este caminho para o diretório onde você extraiu o dataset DROZY\n",
    "# Exemplo: Se o seu notebook está em 'projeto_fadiga/notebooks/' e o DROZY em 'projeto_fadiga/DROZY/',\n",
    "# o caminho relativo seria '../DROZY/'\n",
    "DROZY_BASE_PATH = 'DROZY' # Caminho base para o diretório raiz do dataset DROZY\n",
    "\n",
    "# Verificar se o caminho existe\n",
    "if not os.path.exists(DROZY_BASE_PATH):\n",
    "    print(f\"ATENÇÃO: O caminho '{DROZY_BASE_PATH}' não foi encontrado.\")\n",
    "    print(\"Por favor, ajuste 'DROZY_BASE_PATH' para o diretório correto do dataset DROZY.\")\n",
    "else:\n",
    "    print(f\"Caminho base do DROZY definido para: {DROZY_BASE_PATH}\")\n",
    "\n",
    "# Caminhos específicos para os subdiretórios do DROZY\n",
    "VIDEOS_PATH = os.path.join(DROZY_BASE_PATH, 'videos_i8')\n",
    "TIMESTAMPS_PATH = os.path.join(DROZY_BASE_PATH, 'timestamps')\n",
    "KSS_PATH = os.path.join(DROZY_BASE_PATH, 'KSS.txt')\n",
    "PVT_RT_PATH = os.path.join(DROZY_BASE_PATH, 'pvt-rt')\n",
    "ANNOTATIONS_AUTO_PATH = os.path.join(DROZY_BASE_PATH, 'annotations-auto')\n",
    "INTERP_INDICES_PATH = os.path.join(DROZY_BASE_PATH, 'interpIndices')\n",
    "\n",
    "\n",
    "# --- Carregar modelos pré-treinados para detecção de face e landmarks ---\n",
    "# Dlib é uma boa escolha para ambos, e os modelos são bem otimizados.\n",
    "# Baixe esses arquivos e coloque-os em uma pasta 'models' (ou ajuste o caminho)\n",
    "# Arquivo shape_predictor_68_face_landmarks.dat deve ser baixado de:\n",
    "# http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 (descompactar)\n",
    "# Arquivo mmod_human_face_detector.dat (opcional, para detecção de face de maior qualidade)\n",
    "# http://dlib.net/files/mmod_human_face_detector.dat.bz2 (descompactar)\n",
    "\n",
    "MODELS_DIR = './models/' # Caminho para a pasta onde você guardará os modelos dlib\n",
    "\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "    os.makedirs(MODELS_DIR)\n",
    "    print(f\"Criado diretório para modelos: {MODELS_DIR}\")\n",
    "\n",
    "PREDICTOR_PATH = os.path.join(MODELS_DIR, 'shape_predictor_68_face_landmarks.dat')\n",
    "FACE_DETECTOR_PATH = os.path.join(MODELS_DIR, 'mmod_human_face_detector.dat') # Opcional, dlib tem um detector de face padrão mais rápido\n",
    "\n",
    "# Verificar se os modelos estão disponíveis\n",
    "if not os.path.exists(PREDICTOR_PATH):\n",
    "    print(f\"ATENÇÃO: Arquivo '{PREDICTOR_PATH}' não encontrado.\")\n",
    "    print(\"Por favor, baixe 'shape_predictor_68_face_landmarks.dat' e coloque-o na pasta '{MODELS_DIR}'.\")\n",
    "    predictor = None\n",
    "else:\n",
    "    predictor = dlib.shape_predictor(PREDICTOR_PATH)\n",
    "    print(\"Shape predictor de dlib carregado.\")\n",
    "\n",
    "# O detector de face padrão do dlib é 'dlib.get_frontal_face_detector()',\n",
    "# que é rápido e geralmente suficiente. O MMOD é mais preciso, mas mais lento.\n",
    "# Vamos usar o frontal_face_detector para este projeto por padrão, a menos que o MMOD seja explicitamente preferido\n",
    "# para casos de uso mais desafiadores.\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "# face_detector = dlib.cnn_face_detection_model_v1(FACE_DETECTOR_PATH) # Se preferir o detector CNN (mais lento)\n",
    "print(\"Detector de face de dlib (frontal) carregado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664440bb-bb8c-4c36-9319-17793c25dd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Visão Geral do Dataset DROZY ---\n",
      "O DROZY é um dataset multimodal para detecção de fadiga. Ele inclui:\n",
      "- Vídeos de intensidade (8-bit) em: DROZY/videos_i8\n",
      "- Timestamps por frame em: DROZY/timestamps\n",
      "- Scores da Karolinska Sleepiness Scale (KSS) em: DROZY/KSS.txt\n",
      "- Tempos de Reação do Psychomotor Vigilance Test (PVT-RT) em: DROZY/pvt-rt\n",
      "- Anotações automáticas de 68 landmarks faciais 2D/3D em: DROZY/annotations-auto\n",
      "- Índices de interpolação para frames perdidos em: DROZY/interpIndices\n",
      "\n",
      "O objetivo é utilizar KSS e/ou PVT-RT como nosso 'ground truth' para o estado de fadiga.\n",
      "\n",
      "--- Carregando KSS Scores ---\n",
      "KSS Scores carregados.\n",
      "Amostra do DataFrame KSS:\n",
      "   Subject  Test  KSS_Score\n",
      "0        1     1        3.0\n",
      "1        1     2        6.0\n",
      "2        1     3        7.0\n",
      "3        2     1        3.0\n",
      "4        2     2        7.0\n",
      "\n",
      "Distribuição dos KSS Scores:\n",
      "KSS_Score\n",
      "0.0    1\n",
      "2.0    5\n",
      "3.0    7\n",
      "4.0    4\n",
      "5.0    2\n",
      "6.0    7\n",
      "7.0    9\n",
      "8.0    5\n",
      "9.0    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuição dos rótulos binários (KSS >= 7):\n",
      "Fatigue_Label_KSS\n",
      "0    0.619048\n",
      "1    0.380952\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Carregando PVT Reaction Times (PVT-RT) ---\n",
      "Nenhum dado PVT-RT carregado. Verifique o caminho ou a estrutura dos arquivos.\n",
      "\n",
      "Preparação inicial dos dados concluída. Próximo passo: processar vídeos e extrair características.\n"
     ]
    }
   ],
   "source": [
    "# Célula 2: Visão Geral e Carregamento dos Dados KSS e PVT-RT (Ground Truth)\n",
    "\n",
    "print(\"--- Visão Geral do Dataset DROZY ---\")\n",
    "print(\"O DROZY é um dataset multimodal para detecção de fadiga. Ele inclui:\")\n",
    "print(f\"- Vídeos de intensidade (8-bit) em: {VIDEOS_PATH}\")\n",
    "print(f\"- Timestamps por frame em: {TIMESTAMPS_PATH}\")\n",
    "print(f\"- Scores da Karolinska Sleepiness Scale (KSS) em: {KSS_PATH}\")\n",
    "print(f\"- Tempos de Reação do Psychomotor Vigilance Test (PVT-RT) em: {PVT_RT_PATH}\")\n",
    "print(f\"- Anotações automáticas de 68 landmarks faciais 2D/3D em: {ANNOTATIONS_AUTO_PATH}\")\n",
    "print(f\"- Índices de interpolação para frames perdidos em: {INTERP_INDICES_PATH}\")\n",
    "print(\"\\nO objetivo é utilizar KSS e/ou PVT-RT como nosso 'ground truth' para o estado de fadiga.\")\n",
    "\n",
    "# --- Carregamento e Exploração dos Dados KSS ---\n",
    "print(\"\\n--- Carregando KSS Scores ---\")\n",
    "try:\n",
    "    kss_df = pd.read_csv(KSS_PATH, header=None, sep=r'\\s+', names=['Subject_1_Test_1', 'Subject_1_Test_2', 'Subject_1_Test_3'])\n",
    "\n",
    "    # O KSS.txt tem uma estrutura peculiar, vamos reconstruí-la para ser mais utilizável\n",
    "    # O README diz: \"14 lines (1 per subject) and 3 columns (1 per test)\"\n",
    "    # Mas a estrutura parece ser mais complexa. Vamos tentar ler sem cabeçalho e transpor,\n",
    "    # Ou, idealmente, se houvesse um CSV mais estruturado.\n",
    "    # Dado o README, parece que cada linha representa um sujeito e cada coluna um teste.\n",
    "    # No entanto, o KSS é uma escala subjetiva preenchida ao final de cada teste.\n",
    "    # A maneira mais segura é tratar cada combinação Sujeito-Teste como uma entrada.\n",
    "\n",
    "    # Vamos assumir que as linhas são sujeitos (1-14) e as colunas são testes (1-3)\n",
    "    # E o teste 7-1 tem KSS 0 (arbitrário, pois não ocorreu)\n",
    "    \n",
    "    # KSS_data = np.loadtxt(KSS_PATH)\n",
    "    # subjects = np.arange(1, 15)\n",
    "    # tests = np.arange(1, 4)\n",
    "    # kss_list = []\n",
    "    # for i, subj in enumerate(subjects):\n",
    "    #     for j, test in enumerate(tests):\n",
    "    #         # Teste 7-1 é uma exceção\n",
    "    #         if subj == 7 and test == 1:\n",
    "    #             kss_score = 0\n",
    "    #         else:\n",
    "    #             kss_score = KSS_data[i, j]\n",
    "    #         kss_list.append({'Subject': subj, 'Test': test, 'KSS_Score': kss_score})\n",
    "    # kss_df = pd.DataFrame(kss_list)\n",
    "    \n",
    "    # Reavaliando o KSS.txt com base na descrição (14 linhas, 3 colunas)\n",
    "    # Parece que cada linha é um sujeito e cada coluna é um teste (1, 2, 3)\n",
    "    kss_raw_data = np.loadtxt(KSS_PATH)\n",
    "    \n",
    "    kss_records = []\n",
    "    for s_idx in range(kss_raw_data.shape[0]):\n",
    "        for t_idx in range(kss_raw_data.shape[1]):\n",
    "            subject_id = s_idx + 1\n",
    "            test_id = t_idx + 1\n",
    "            kss_score = kss_raw_data[s_idx, t_idx]\n",
    "            \n",
    "            # Ajustar para o teste 7-1 que não ocorreu e tem KSS 0\n",
    "            if subject_id == 7 and test_id == 1:\n",
    "                kss_score = 0 # Conforme README\n",
    "            \n",
    "            kss_records.append({'Subject': subject_id, 'Test': test_id, 'KSS_Score': kss_score})\n",
    "            \n",
    "    kss_df = pd.DataFrame(kss_records)\n",
    "\n",
    "    print(\"KSS Scores carregados.\")\n",
    "    print(\"Amostra do DataFrame KSS:\")\n",
    "    print(kss_df.head())\n",
    "    print(f\"\\nDistribuição dos KSS Scores:\\n{kss_df['KSS_Score'].value_counts().sort_index()}\")\n",
    "\n",
    "    # --- Definição do Ground Truth Binário a partir do KSS ---\n",
    "    # Fadiga pode ser definida como KSS >= 7 (sonolência clara)\n",
    "    # KSS 1-6 = Não Fadigado, KSS 7-9 = Fadigado\n",
    "    KSS_FATIGUE_THRESHOLD = 7\n",
    "    kss_df['Fatigue_Label_KSS'] = (kss_df['KSS_Score'] >= KSS_FATIGUE_THRESHOLD).astype(int)\n",
    "    print(f\"\\nDistribuição dos rótulos binários (KSS >= {KSS_FATIGUE_THRESHOLD}):\\n{kss_df['Fatigue_Label_KSS'].value_counts(normalize=True)}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivo KSS.txt não encontrado em '{KSS_PATH}'. Verifique o caminho.\")\n",
    "    kss_df = None\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar ou processar KSS.txt: {e}\")\n",
    "    kss_df = None\n",
    "\n",
    "\n",
    "# --- Carregamento e Exploração dos Dados PVT-RT ---\n",
    "print(\"\\n--- Carregando PVT Reaction Times (PVT-RT) ---\")\n",
    "# O PVT-RT está em arquivos .csv separados por Subject-Test\n",
    "# Vamos carregar um exemplo para entender a estrutura\n",
    "pvt_data = {}\n",
    "for subject_folder in sorted(os.listdir(PVT_RT_PATH)):\n",
    "    if subject_folder.startswith('Subject'):\n",
    "        subject_id = int(subject_folder.replace('Subject', ''))\n",
    "        pvt_data[subject_id] = {}\n",
    "        subject_pvt_path = os.path.join(PVT_RT_PATH, subject_folder)\n",
    "        \n",
    "        # O README indica que os arquivos estão diretamente em pvt-rt/SUBJECT-TEST.csv\n",
    "        # Na verdade, é SUBJECT-TEST.csv (ex: 1-1.csv)\n",
    "        # Vamos listar todos os arquivos csv e extrair subject e test id\n",
    "        \n",
    "        for file_name in os.listdir(PVT_RT_PATH):\n",
    "            if file_name.endswith('.csv'):\n",
    "                try:\n",
    "                    parts = file_name.replace('.csv', '').split('-')\n",
    "                    s_id = int(parts[0])\n",
    "                    t_id = int(parts[1])\n",
    "                    \n",
    "                    file_path = os.path.join(PVT_RT_PATH, file_name)\n",
    "                    df_pvt_test = pd.read_csv(file_path, sep=';', header=None, skiprows=1) # skip header linha 1\n",
    "                    \n",
    "                    # A 1ª coluna é o timestamp do estímulo, a 2ª é o timestamp da reação\n",
    "                    # RT = tempo de reação (coluna 1 - coluna 0)\n",
    "                    reaction_times = (df_pvt_test.iloc[:, 1] - df_pvt_test.iloc[:, 0]).values\n",
    "                    \n",
    "                    if s_id not in pvt_data:\n",
    "                        pvt_data[s_id] = {}\n",
    "                    pvt_data[s_id][t_id] = reaction_times\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao carregar PVT-RT para {file_name}: {e}\")\n",
    "                    continue\n",
    "# Imprimir um exemplo\n",
    "if pvt_data:\n",
    "    first_subj = list(pvt_data.keys())[0]\n",
    "    first_test = list(pvt_data[first_subj].keys())[0]\n",
    "    print(f\"Exemplo de PVT-RT para Sujeito {first_subj}, Teste {first_test}:\")\n",
    "    print(f\"Número de reações: {len(pvt_data[first_subj][first_test])}\")\n",
    "    print(f\"Média de RT: {np.mean(pvt_data[first_subj][first_test]):.2f} ms\")\n",
    "    print(f\"Desvio Padrão de RT: {np.std(pvt_data[first_subj][first_test]):.2f} ms\")\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(pvt_data[first_subj][first_test], bins=30, kde=True)\n",
    "    plt.title(f'Distribuição dos Tempos de Reação PVT (Sujeito {first_subj}, Teste {first_test})')\n",
    "    plt.xlabel('Tempo de Reação (ms)')\n",
    "    plt.ylabel('Frequência')\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Definição do Ground Truth Binário a partir do PVT-RT ---\n",
    "    # Uma forma comum é usar a média ou a mediana dos RTs.\n",
    "    # Tempos de reação mais longos indicam fadiga.\n",
    "    # Por agora, vamos apenas demonstrar o carregamento. A binarização exata será decidida\n",
    "    # na fase de extração de características por janela temporal, onde o PVT-RT médio\n",
    "    # para aquela janela será usado.\n",
    "    print(\"\\nPVT-RTs carregados. A binarização dos rótulos de fadiga a partir de PVT-RTs será feita\")\n",
    "    print(\"durante a extração de características por janela temporal no próximo passo.\")\n",
    "else:\n",
    "    print(\"Nenhum dado PVT-RT carregado. Verifique o caminho ou a estrutura dos arquivos.\")\n",
    "\n",
    "\n",
    "print(\"\\nPreparação inicial dos dados concluída. Próximo passo: processar vídeos e extrair características.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ceb2750-e73a-4e7e-96dd-05d0bc912646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processando Vídeos e Extraindo Características Faciais ---\n",
      "Encontradas 36 combinações Sujeito-Teste.\n",
      "\n",
      "Processando vídeo: Sujeito 1, Teste 1 (1-1.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 1, Teste 2 (1-2.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 1, Teste 3 (1-3.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 2, Teste 1 (2-1.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 2, Teste 2 (2-2.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 2, Teste 3 (2-3.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 3, Teste 1 (3-1.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 3, Teste 2 (3-2.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 3, Teste 3 (3-3.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 4, Teste 1 (4-1.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 4, Teste 2 (4-2.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 4, Teste 3 (4-3.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 5, Teste 1 (5-1.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 5, Teste 2 (5-2.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 5, Teste 3 (5-3.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 6, Teste 1 (6-1.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 6, Teste 2 (6-2.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 6, Teste 3 (6-3.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 7, Teste 2 (7-2.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 7, Teste 3 (7-3.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 8, Teste 1 (8-1.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 8, Teste 2 (8-2.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 8, Teste 3 (8-3.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 9, Teste 2 (9-2.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 9, Teste 3 (9-3.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 10, Teste 1 (10-1.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 10, Teste 3 (10-3.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 11, Teste 1 (11-1.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 11, Teste 2 (11-2.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 11, Teste 3 (11-3.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 12, Teste 1 (12-1.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 13, Teste 1 (13-1.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 13, Teste 2 (13-2.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 14, Teste 1 (14-1.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 14, Teste 2 (14-2.mp4)\n",
      "\n",
      "Processando vídeo: Sujeito 14, Teste 3 (14-3.mp4)\n",
      "\n",
      "Processamento de vídeos (ou até o limite de teste) concluído.\n",
      "Total de entradas na lista de características: 497609\n",
      "\n",
      "--- Amostra do DataFrame de Características Extraídas ---\n",
      "   Subject  Test  Frame_Index  Elapsed_Time_ms       EAR       MAR  \\\n",
      "0        1     1            0               23  0.332758  0.837950   \n",
      "1        1     1            1               57  0.348747  0.953561   \n",
      "2        1     1            2               89  0.360704  0.946596   \n",
      "3        1     1            3              124  0.350787  0.843328   \n",
      "4        1     1            4              157  0.374542  0.894133   \n",
      "\n",
      "   Fatigue_Label  Face_Detected  \n",
      "0              0           True  \n",
      "1              0           True  \n",
      "2              0           True  \n",
      "3              0           True  \n",
      "4              0           True  \n",
      "\n",
      "Total de frames processados no DataFrame: 497609\n",
      "Porcentagem de frames com face detectada: 99.65%\n",
      "\n",
      "Características extraídas salvas em: processed_data/all_video_features.pkl\n",
      "\n",
      "Célula 3 concluída. Os dados agora estão prontos para serem transformados em sequências para os modelos.\n"
     ]
    }
   ],
   "source": [
    "# Célula 3: Processamento de Vídeos, Extração de Landmarks e Cálculo de Métricas (EAR, MAR)\n",
    "\n",
    "print(\"--- Processando Vídeos e Extraindo Características Faciais ---\")\n",
    "\n",
    "# --- Funções Auxiliares para Detecção de Landmarks e Cálculo de EAR/MAR ---\n",
    "\n",
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "    # Converte o objeto de shape de dlib em um array NumPy\n",
    "    coords = np.zeros((68, 2), dtype=dtype)\n",
    "    for i in range(0, 68):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "    return coords\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    # Calcula a Eye Aspect Ratio (EAR)\n",
    "    # Distâncias euclidianas entre os dois conjuntos de marcos verticais dos olhos (y-coord)\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    # Distância euclidiana entre o conjunto de marcos horizontais dos olhos (x-coord)\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    # Calcula a EAR\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "def mouth_aspect_ratio(mouth):\n",
    "    # Calcula a Mouth Aspect Ratio (MAR) para detecção de bocejos\n",
    "    # Distâncias euclidianas entre os marcos verticais da boca\n",
    "    A = np.linalg.norm(mouth[1] - mouth[7]) # 51-59 ou 61-67\n",
    "    B = np.linalg.norm(mouth[2] - mouth[6]) # 52-58 ou 62-66\n",
    "    C = np.linalg.norm(mouth[3] - mouth[5]) # 53-57 ou 63-65 (pontos internos, se disponíveis, ou externos)\n",
    "    # Distância euclidiana entre os marcos horizontais da boca\n",
    "    D = np.linalg.norm(mouth[0] - mouth[4]) # 48-54 ou 60-64\n",
    "\n",
    "    # Calcula a MAR. Usamos a média das distâncias verticais dividido pela horizontal\n",
    "    mar = (A + B + C) / (3.0 * D) \n",
    "    return mar\n",
    "\n",
    "# Índices para os pontos de referência faciais (dlib 68 landmarks)\n",
    "# Olho esquerdo: 36-41\n",
    "# Olho direito: 42-47\n",
    "# Boca: 48-67\n",
    "LEFT_EYE_START, LEFT_EYE_END = 36, 42\n",
    "RIGHT_EYE_START, RIGHT_EYE_END = 42, 48\n",
    "MOUTH_START, MOUTH_END = 48, 68\n",
    "\n",
    "# --- Lista de Sujeitos e Testes no Dataset DROZY ---\n",
    "# Vamos inferir os IDs de Sujeitos e Testes a partir da estrutura de arquivos de vídeo\n",
    "subject_test_combinations = []\n",
    "if os.path.exists(VIDEOS_PATH):\n",
    "    for video_file in os.listdir(VIDEOS_PATH):\n",
    "        if video_file.endswith('.mp4'):\n",
    "            try:\n",
    "                # Exemplo: 1-1.mp4 -> Subject 1, Test 1\n",
    "                parts = os.path.splitext(video_file)[0].split('-')\n",
    "                subject_id = int(parts[0])\n",
    "                test_id = int(parts[1])\n",
    "                subject_test_combinations.append((subject_id, test_id))\n",
    "            except Exception as e:\n",
    "                print(f\"Não foi possível parsear o nome do arquivo de vídeo {video_file}: {e}\")\n",
    "    subject_test_combinations = sorted(list(set(subject_test_combinations)))\n",
    "    print(f\"Encontradas {len(subject_test_combinations)} combinações Sujeito-Teste.\")\n",
    "else:\n",
    "    print(f\"Erro: Caminho de vídeos '{VIDEOS_PATH}' não encontrado. Não é possível inferir Sujeitos/Testes.\")\n",
    "    subject_test_combinations = [] # Garante que a lista esteja vazia se o caminho não existe\n",
    "\n",
    "# --- Dicionário para armazenar todas as características extraídas ---\n",
    "all_extracted_features = [] # List de dicionários, cada um para um frame\n",
    "\n",
    "# --- Loop Principal para Processar Cada Vídeo (Sujeito-Teste) ---\n",
    "# Limite para um número menor de vídeos para demonstração rápida. Remova em produção.\n",
    "# MAX_VIDEOS_TO_PROCESS = 5 # Processar apenas os primeiros 5 vídeos para teste\n",
    "# processed_videos_count = 0\n",
    "\n",
    "for subject_id, test_id in subject_test_combinations:\n",
    "    # if processed_videos_count >= MAX_VIDEOS_TO_PROCESS: # Ativar limite\n",
    "    #    print(f\"Limite de {MAX_VIDEOS_TO_PROCESS} vídeos atingido para demonstração.\")\n",
    "    #    break\n",
    "\n",
    "    video_filename = f\"{subject_id}-{test_id}.mp4\"\n",
    "    video_path = os.path.join(VIDEOS_PATH, video_filename)\n",
    "    timestamps_path = os.path.join(TIMESTAMPS_PATH, f\"{subject_id}-{test_id}.txt\")\n",
    "    \n",
    "    # interp_indices_path não está sendo usado neste loop, então não precisa carregar aqui.\n",
    "    # interp_indices_path = os.path.join(INTERP_INDICES_PATH, f\"{subject_id}-{test_id}.txt\")\n",
    "\n",
    "    if not os.path.exists(video_path) or not os.path.exists(timestamps_path):\n",
    "        print(f\"Pulando {video_filename}: Arquivo de vídeo ou timestamps não encontrado.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessando vídeo: Sujeito {subject_id}, Teste {test_id} ({video_filename})\")\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Não foi possível abrir o vídeo: {video_path}\")\n",
    "        continue\n",
    "\n",
    "    # Carregar timestamps\n",
    "    timestamps_df = pd.read_csv(timestamps_path, header=None, sep=r'\\s+',\n",
    "                                names=['year', 'month', 'day', 'hours', 'minutes', 'seconds', 'milliseconds', 'elapsed_time_ms'])\n",
    "    \n",
    "    # Carregar KSS Score para este teste (se kss_df foi carregado corretamente na Célula 2)\n",
    "    current_kss_score_row = kss_df[(kss_df['Subject'] == subject_id) & (kss_df['Test'] == test_id)]\n",
    "    if not current_kss_score_row.empty:\n",
    "        current_fatigue_label = current_kss_score_row['Fatigue_Label_KSS'].values[0]\n",
    "    else:\n",
    "        current_fatigue_label = -1 # Marcador para KSS não encontrado\n",
    "        print(f\"Aviso: KSS Score não encontrado para Sujeito {subject_id}, Teste {test_id}. Marcando frames com -1.\")\n",
    "        # Pode optar por pular o vídeo se o ground truth for crucial e ausente:\n",
    "        # cap.release()\n",
    "        # continue\n",
    "\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break # Fim do vídeo\n",
    "\n",
    "        if frame_idx >= len(timestamps_df):\n",
    "            print(f\"Aviso: Mais frames que timestamps para {video_filename}. Parando.\")\n",
    "            break\n",
    "        \n",
    "        # Obter o timestamp para o frame atual\n",
    "        current_timestamp_ms = timestamps_df.iloc[frame_idx]['elapsed_time_ms']\n",
    "\n",
    "        # Converter para escala de cinza para dlib\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detecção de face\n",
    "        # face_detector é 'dlib.get_frontal_face_detector()'\n",
    "        faces = face_detector(gray, 0) # 0 = nenhum upsampling\n",
    "        \n",
    "        if len(faces) == 0:\n",
    "            # print(f\"Nenhuma face detectada no frame {frame_idx} de {video_filename}\") # Descomente para depuração\n",
    "            # Se nenhuma face for detectada, adicione um registro com valores padrão e Face_Detected = False\n",
    "            all_extracted_features.append({\n",
    "                'Subject': subject_id,\n",
    "                'Test': test_id,\n",
    "                'Frame_Index': frame_idx,\n",
    "                'Elapsed_Time_ms': current_timestamp_ms,\n",
    "                'EAR': 0.0, # Pode usar np.nan para indicar ausência real\n",
    "                'MAR': 0.0, # Pode usar np.nan\n",
    "                'Fatigue_Label': current_fatigue_label,\n",
    "                'Face_Detected': False\n",
    "            })\n",
    "            frame_idx += 1\n",
    "            continue\n",
    "        \n",
    "        # Considerando a maior face detectada se houver múltiplas\n",
    "        # Para dlib.get_frontal_face_detector(), o objeto retornado é um rect diretamente\n",
    "        face = max(faces, key=lambda rect: rect.width() * rect.height()) \n",
    "\n",
    "        # Detecção de 68 landmarks faciais\n",
    "        shape = predictor(gray, face)\n",
    "        shape_np = shape_to_np(shape)\n",
    "\n",
    "        # Extração de características (EAR, MAR)\n",
    "        left_eye = shape_np[LEFT_EYE_START:LEFT_EYE_END]\n",
    "        right_eye = shape_np[RIGHT_EYE_START:RIGHT_EYE_END]\n",
    "        mouth = shape_np[MOUTH_START:MOUTH_END]\n",
    "\n",
    "        ear = (eye_aspect_ratio(left_eye) + eye_aspect_ratio(right_eye)) / 2.0\n",
    "        mar = mouth_aspect_ratio(mouth)\n",
    "        \n",
    "        # Adicionar as características extraídas e o ground truth à lista\n",
    "        all_extracted_features.append({\n",
    "            'Subject': subject_id,\n",
    "            'Test': test_id,\n",
    "            'Frame_Index': frame_idx,\n",
    "            'Elapsed_Time_ms': current_timestamp_ms,\n",
    "            'EAR': ear,\n",
    "            'MAR': mar,\n",
    "            'Fatigue_Label': current_fatigue_label,\n",
    "            'Face_Detected': True # Indica que a face foi detectada\n",
    "        })\n",
    "\n",
    "        # Opcional: Visualização de um frame (descomente para depuração)\n",
    "        #if frame_idx % 30 == 0: # A cada segundo (se 30 fps)\n",
    "        #    frame_display = frame.copy()\n",
    "        #    cv2.rectangle(frame_display, (face.left(), face.top()), (face.right(), face.bottom()), (0, 255, 0), 2)\n",
    "        #    for (x, y) in shape_np:\n",
    "        #        cv2.circle(frame_display, (x, y), 1, (0, 0, 255), -1)\n",
    "        #    cv2.putText(frame_display, f\"EAR: {ear:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        #    cv2.putText(frame_display, f\"MAR: {mar:.2f}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        #    cv2.putText(frame_display, f\"KSS Label: {current_fatigue_label}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "        #     \n",
    "        #     # Converta de BGR para RGB para exibir no matplotlib\n",
    "        #    plt.imshow(cv2.cvtColor(frame_display, cv2.COLOR_BGR2RGB))\n",
    "        #    plt.title(f\"S{subject_id} T{test_id} - Frame {frame_idx}\")\n",
    "        #    plt.axis('off')\n",
    "        #    plt.show()\n",
    "\n",
    "        frame_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "    # processed_videos_count += 1 # Ativar limite\n",
    "\n",
    "print(\"\\nProcessamento de vídeos (ou até o limite de teste) concluído.\")\n",
    "print(f\"Total de entradas na lista de características: {len(all_extracted_features)}\")\n",
    "\n",
    "# Converter a lista de dicionários em um DataFrame Pandas\n",
    "df_features = pd.DataFrame(all_extracted_features)\n",
    "\n",
    "print(\"\\n--- Amostra do DataFrame de Características Extraídas ---\")\n",
    "print(df_features.head())\n",
    "print(f\"\\nTotal de frames processados no DataFrame: {len(df_features)}\")\n",
    "\n",
    "# --- Verificação e Tratamento da Coluna 'Face_Detected' ---\n",
    "if not df_features.empty and 'Face_Detected' in df_features.columns:\n",
    "    print(f\"Porcentagem de frames com face detectada: {df_features['Face_Detected'].mean():.2%}\")\n",
    "else:\n",
    "    print(\"Aviso: DataFrame de características está vazio ou a coluna 'Face_Detected' não foi criada.\")\n",
    "    print(\"Isso pode indicar que nenhum vídeo foi processado ou que a detecção facial falhou em todos os frames.\")\n",
    "\n",
    "# --- Tratamento de Outliers e Valores Ausentes (EAR/MAR=0.0 quando face não detectada) ---\n",
    "# Decide como lidar com frames onde a face não foi detectada.\n",
    "# Para o Modelo 1 e Modelo 2, pode ser útil manter 0.0s ou substituí-los por np.nan para um tratamento explícito\n",
    "# de valores ausentes (e.g., com interpolação, ou um valor de preenchimento).\n",
    "# Por agora, mantemos 0.0, mas este é um ponto para refinar na próxima célula ou nos notebooks dos modelos.\n",
    "\n",
    "# Salvar o DataFrame de características para uso nos notebooks dos modelos\n",
    "OUTPUT_DATA_DIR = 'processed_data' # Garante que está no mesmo nível dos notebooks\n",
    "OUTPUT_FEATURES_FILE = os.path.join(OUTPUT_DATA_DIR, 'all_video_features.pkl')\n",
    "os.makedirs(OUTPUT_DATA_DIR, exist_ok=True) # Garante que o diretório de saída existe\n",
    "dump(df_features, OUTPUT_FEATURES_FILE)\n",
    "print(f\"\\nCaracterísticas extraídas salvas em: {OUTPUT_FEATURES_FILE}\")\n",
    "\n",
    "print(\"\\nCélula 3 concluída. Os dados agora estão prontos para serem transformados em sequências para os modelos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3905acb9-2fde-4188-ba10-6946e869c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Transformando dados de frame em sequências para treinamento dos modelos ---\n",
      "Ocorreu um erro ao carregar o DataFrame de características: invalid load key, '\\x10'.\n",
      "\n",
      "Parâmetros de Sequência:\n",
      "  Duração da Sequência: 3 segundos (90 frames)\n",
      "  Sobreposição de Sequência: 50.0% (45 frames por passo)\n",
      "  Características utilizadas: EAR, MAR\n",
      "Gerando sequências para Sujeito: 1, Teste: 1 (Total de frames: 17865)\n",
      "Gerando sequências para Sujeito: 1, Teste: 2 (Total de frames: 9497)\n",
      "Gerando sequências para Sujeito: 1, Teste: 3 (Total de frames: 8866)\n",
      "Gerando sequências para Sujeito: 2, Teste: 1 (Total de frames: 17899)\n",
      "Gerando sequências para Sujeito: 2, Teste: 2 (Total de frames: 9401)\n",
      "Gerando sequências para Sujeito: 2, Teste: 3 (Total de frames: 8146)\n",
      "Gerando sequências para Sujeito: 3, Teste: 1 (Total de frames: 17882)\n",
      "Gerando sequências para Sujeito: 3, Teste: 2 (Total de frames: 8848)\n",
      "Gerando sequências para Sujeito: 3, Teste: 3 (Total de frames: 8874)\n",
      "Gerando sequências para Sujeito: 4, Teste: 1 (Total de frames: 17789)\n",
      "Gerando sequências para Sujeito: 4, Teste: 2 (Total de frames: 8463)\n",
      "Gerando sequências para Sujeito: 4, Teste: 3 (Total de frames: 8889)\n",
      "Gerando sequências para Sujeito: 5, Teste: 1 (Total de frames: 17898)\n",
      "Gerando sequências para Sujeito: 5, Teste: 2 (Total de frames: 8175)\n",
      "Gerando sequências para Sujeito: 5, Teste: 3 (Total de frames: 8230)\n",
      "Gerando sequências para Sujeito: 6, Teste: 1 (Total de frames: 17913)\n",
      "Gerando sequências para Sujeito: 6, Teste: 2 (Total de frames: 8852)\n",
      "Gerando sequências para Sujeito: 6, Teste: 3 (Total de frames: 8957)\n",
      "Gerando sequências para Sujeito: 7, Teste: 2 (Total de frames: 7912)\n",
      "Gerando sequências para Sujeito: 7, Teste: 3 (Total de frames: 8878)\n",
      "Gerando sequências para Sujeito: 8, Teste: 1 (Total de frames: 17863)\n",
      "Gerando sequências para Sujeito: 8, Teste: 2 (Total de frames: 8874)\n",
      "Gerando sequências para Sujeito: 8, Teste: 3 (Total de frames: 9025)\n",
      "Gerando sequências para Sujeito: 9, Teste: 2 (Total de frames: 17908)\n",
      "Gerando sequências para Sujeito: 9, Teste: 3 (Total de frames: 17922)\n",
      "Gerando sequências para Sujeito: 10, Teste: 1 (Total de frames: 17866)\n",
      "Gerando sequências para Sujeito: 10, Teste: 3 (Total de frames: 17889)\n",
      "Gerando sequências para Sujeito: 11, Teste: 1 (Total de frames: 17914)\n",
      "Gerando sequências para Sujeito: 11, Teste: 2 (Total de frames: 17886)\n",
      "Gerando sequências para Sujeito: 11, Teste: 3 (Total de frames: 17875)\n",
      "Gerando sequências para Sujeito: 12, Teste: 1 (Total de frames: 17889)\n",
      "Gerando sequências para Sujeito: 13, Teste: 1 (Total de frames: 17902)\n",
      "Gerando sequências para Sujeito: 13, Teste: 2 (Total de frames: 17900)\n",
      "Gerando sequências para Sujeito: 14, Teste: 1 (Total de frames: 17883)\n",
      "Gerando sequências para Sujeito: 14, Teste: 2 (Total de frames: 17861)\n",
      "Gerando sequências para Sujeito: 14, Teste: 3 (Total de frames: 17918)\n",
      "\n",
      "--- Finalizando Geração de Sequências e Features Agregadas ---\n",
      "Formato final das sequências para TCN (X_sequences): (11006, 90, 2)\n",
      "Formato final dos rótulos para TCN (y_labels_tcn): (11006,)\n",
      "\n",
      "Formato final das características para Modelo 1 (X_model1_features): (11006, 4)\n",
      "Formato final dos rótulos para Modelo 1 (y_labels_model1): (11006,)\n",
      "\n",
      "--- Salvando Dados Processados para os Modelos ---\n",
      "Dados para Modelo 1 salvos em: processed_data/model1_features_labels.pkl\n",
      "Dados para Modelo 2 (TCN) salvos em: processed_data/model2_sequences_labels.pkl\n",
      "\n",
      "Célula 4 concluída. Os datasets para cada modelo foram preparados e salvos.\n",
      "Agora você pode prosseguir para os notebooks de treinamento de cada modelo.\n"
     ]
    }
   ],
   "source": [
    "# Célula 4: Geração de Dataset para Treinamento (Preparação Final - Sequências e Amostras)\n",
    "\n",
    "OUTPUT_DATA_DIR = 'processed_data'\n",
    "\n",
    "print(\"--- Transformando dados de frame em sequências para treinamento dos modelos ---\")\n",
    "\n",
    "# Carregar o DataFrame de características salvo na Célula 3\n",
    "try:\n",
    "    df_features = pd.read_pickle(os.path.join(OUTPUT_DATA_DIR, 'all_video_features.pkl'))\n",
    "    print(f\"DataFrame de características carregado. Total de frames: {len(df_features)}\")\n",
    "    if df_features.empty:\n",
    "        print(\"Aviso: O DataFrame de características está vazio. Não há dados para formar sequências.\")\n",
    "        # Pode adicionar um exit() ou raise uma exceção aqui se o projeto não puder continuar\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivo 'all_video_features.pkl' não encontrado em '{OUTPUT_DATA_DIR}'.\")\n",
    "    print(\"Por favor, execute a Célula 3 primeiro.\")\n",
    "    # exit() # Ou outro tratamento de erro\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao carregar o DataFrame de características: {e}\")\n",
    "    # exit()\n",
    "\n",
    "# --- Definição dos Parâmetros de Sequência ---\n",
    "# Estes parâmetros são cruciais e devem ser ajustados experimentalmente.\n",
    "# Eles afetam diretamente como os modelos percebem a fadiga temporal.\n",
    "\n",
    "SEQUENCE_LENGTH_SECONDS = 3  # Duração da sequência em segundos (e.g., 5 segundos de dados)\n",
    "FPS = 30                     # Frames por segundo do vídeo (DROZY videos_i8 é 30 FPS)\n",
    "SEQUENCE_LENGTH_FRAMES = SEQUENCE_LENGTH_SECONDS * FPS # Número de frames em cada sequência\n",
    "\n",
    "OVERLAP_PERCENTAGE = 0.5     # Porcentagem de sobreposição entre sequências (e.g., 0.5 = 50% de sobreposição)\n",
    "STEP_SIZE_FRAMES = int(SEQUENCE_LENGTH_FRAMES * (1 - OVERLAP_PERCENTAGE))\n",
    "\n",
    "# Características que serão usadas como entrada para os modelos\n",
    "# Excluímos 'Subject', 'Test', 'Frame_Index', 'Elapsed_Time_ms', 'Face_Detected' pois são meta-dados ou indicadores.\n",
    "# 'Fatigue_Label' é o rótulo.\n",
    "FEATURES_TO_USE = ['EAR', 'MAR']\n",
    "# Adicione outras características se extraídas (ex: pose da cabeça, etc.)\n",
    "# FEATURES_TO_USE = ['EAR', 'MAR', 'Head_Pitch', 'Head_Yaw', 'Head_Roll']\n",
    "\n",
    "print(f\"\\nParâmetros de Sequência:\")\n",
    "print(f\"  Duração da Sequência: {SEQUENCE_LENGTH_SECONDS} segundos ({SEQUENCE_LENGTH_FRAMES} frames)\")\n",
    "print(f\"  Sobreposição de Sequência: {OVERLAP_PERCENTAGE*100}% ({STEP_SIZE_FRAMES} frames por passo)\")\n",
    "print(f\"  Características utilizadas: {', '.join(FEATURES_TO_USE)}\")\n",
    "\n",
    "# --- Estruturas para armazenar as sequências e rótulos ---\n",
    "all_sequences = [] # Para o Modelo 2 (TCN) - array 3D (num_samples, timesteps, num_features)\n",
    "all_labels = []    # Rótulos correspondentes às sequências\n",
    "\n",
    "# Para o Modelo 1 (Feature-Based):\n",
    "# Será um DataFrame com características agregadas por janela (e.g., média, std dev da EAR/MAR na janela)\n",
    "aggregated_features_for_model1 = []\n",
    "\n",
    "\n",
    "# --- Processamento por Sujeito e Teste para Manter a Coerência Temporal ---\n",
    "# Agrupamos por Sujeito e Teste para garantir que as sequências não misturem dados de diferentes vídeos.\n",
    "grouped = df_features.groupby(['Subject', 'Test'])\n",
    "\n",
    "for (subject, test), group in grouped:\n",
    "    print(f\"Gerando sequências para Sujeito: {subject}, Teste: {test} (Total de frames: {len(group)})\")\n",
    "    \n",
    "    # Garantir que os frames estejam em ordem cronológica\n",
    "    group = group.sort_values(by='Frame_Index').reset_index(drop=True)\n",
    "    \n",
    "    # Extrair os valores das características e o rótulo de fadiga para este grupo/vídeo\n",
    "    current_features = group[FEATURES_TO_USE].values\n",
    "    current_fatigue_label = group['Fatigue_Label'].iloc[0] # Assumimos um único rótulo KSS por teste\n",
    "    \n",
    "    # Para o Modelo 2 (TCN): Gerar sequências deslizantes\n",
    "    for i in range(0, len(group) - SEQUENCE_LENGTH_FRAMES + 1, STEP_SIZE_FRAMES):\n",
    "        sequence = current_features[i : i + SEQUENCE_LENGTH_FRAMES]\n",
    "        \n",
    "        # Opcional: Filtrar sequências que contenham muitos frames sem detecção de face\n",
    "        # Um limiar (e.g., 80% dos frames devem ter face detectada) pode ser aplicado aqui\n",
    "        # if (group['Face_Detected'].iloc[i : i + SEQUENCE_LENGTH_FRAMES].sum() / SEQUENCE_LENGTH_FRAMES) < 0.8:\n",
    "        #     continue # Pular sequência se a qualidade for baixa\n",
    "\n",
    "        all_sequences.append(sequence)\n",
    "        all_labels.append(current_fatigue_label) # O rótulo KSS se aplica a todo o teste/vídeo\n",
    "\n",
    "    # Para o Modelo 1 (Feature-Based): Agregação por Janela (EAR/MAR médio, STD, etc.)\n",
    "    # Podemos também criar janelas e calcular estatísticas descritivas sobre EAR, MAR, etc.\n",
    "    # para o Modelo 1, se não quisermos treiná-lo no nível de frame.\n",
    "    # Se o Modelo 1 for treinado em características por frame, isso não é necessário.\n",
    "    # Mas se quisermos que ele tome decisões sobre janelas, fazemos isso aqui.\n",
    "    \n",
    "    # Exemplo de agregação para o Modelo 1 (se ele for treinado em janelas agregadas):\n",
    "    for i in range(0, len(group) - SEQUENCE_LENGTH_FRAMES + 1, STEP_SIZE_FRAMES):\n",
    "        window_data = group.iloc[i : i + SEQUENCE_LENGTH_FRAMES]\n",
    "        \n",
    "        # Calcular a média e desvio padrão de EAR e MAR na janela\n",
    "        mean_ear = window_data['EAR'].mean()\n",
    "        std_ear = window_data['EAR'].std() if len(window_data['EAR']) > 1 else 0.0 # Evitar div por zero\n",
    "        mean_mar = window_data['MAR'].mean()\n",
    "        std_mar = window_data['MAR'].std() if len(window_data['MAR']) > 1 else 0.0\n",
    "\n",
    "        # Adicionar outras métricas da janela, como PERCLOS, contagem de bocejos\n",
    "        # Estes precisariam de lógica adicional aqui.\n",
    "        # Por exemplo, PERCLOS para a janela:\n",
    "        # EAR_THRESHOLD = 0.25 # Exemplo de limiar para olho fechado\n",
    "        # closed_eye_frames = (window_data['EAR'] < EAR_THRESHOLD).sum()\n",
    "        # perclos = (closed_eye_frames / SEQUENCE_LENGTH_FRAMES) * 100 # % de frames com olho fechado\n",
    "\n",
    "        # Contagem de bocejos (precisa de um limiar para MAR e persistência)\n",
    "        # yawns = (window_data['MAR'] > MAR_THRESHOLD).sum() # Lógica mais complexa para bocejos reais\n",
    "        \n",
    "        aggregated_features_for_model1.append({\n",
    "            'Subject': subject,\n",
    "            'Test': test,\n",
    "            'Sequence_Start_Frame': i,\n",
    "            'Mean_EAR': mean_ear,\n",
    "            'Std_EAR': std_ear,\n",
    "            'Mean_MAR': mean_mar,\n",
    "            'Std_MAR': std_mar,\n",
    "            # 'PERCLOS': perclos, # Se calculado\n",
    "            # 'Yawn_Count': yawns, # Se calculado\n",
    "            'Fatigue_Label': current_fatigue_label # Rótulo KSS para a janela\n",
    "        })\n",
    "\n",
    "print(\"\\n--- Finalizando Geração de Sequências e Features Agregadas ---\")\n",
    "\n",
    "# Converter a lista de sequências para um array NumPy (para TCN)\n",
    "X_sequences = np.array(all_sequences)\n",
    "y_labels_tcn = np.array(all_labels)\n",
    "\n",
    "print(f\"Formato final das sequências para TCN (X_sequences): {X_sequences.shape}\")\n",
    "print(f\"Formato final dos rótulos para TCN (y_labels_tcn): {y_labels_tcn.shape}\")\n",
    "\n",
    "# Converter a lista de features agregadas para um DataFrame Pandas (para Modelo 1)\n",
    "df_model1_features = pd.DataFrame(aggregated_features_for_model1)\n",
    "\n",
    "# X_model1_features será as colunas de características, y_labels_model1 será o Fatigue_Label\n",
    "X_model1_features = df_model1_features[['Mean_EAR', 'Std_EAR', 'Mean_MAR', 'Std_MAR']].values # Ajuste se adicionar mais features\n",
    "y_labels_model1 = df_model1_features['Fatigue_Label'].values\n",
    "\n",
    "print(f\"\\nFormato final das características para Modelo 1 (X_model1_features): {X_model1_features.shape}\")\n",
    "print(f\"Formato final dos rótulos para Modelo 1 (y_labels_model1): {y_labels_model1.shape}\")\n",
    "\n",
    "print(\"\\n--- Salvando Dados Processados para os Modelos ---\")\n",
    "\n",
    "# Salvar dados para o Modelo 1\n",
    "model1_data = {'features': X_model1_features, 'labels': y_labels_model1}\n",
    "dump(model1_data, os.path.join(OUTPUT_DATA_DIR, 'model1_features_labels.pkl'))\n",
    "print(f\"Dados para Modelo 1 salvos em: {os.path.join(OUTPUT_DATA_DIR, 'model1_features_labels.pkl')}\")\n",
    "\n",
    "# Salvar dados para o Modelo 2 (TCN)\n",
    "model2_data = {'sequences': X_sequences, 'labels': y_labels_tcn}\n",
    "dump(model2_data, os.path.join(OUTPUT_DATA_DIR, 'model2_sequences_labels.pkl'))\n",
    "print(f\"Dados para Modelo 2 (TCN) salvos em: {os.path.join(OUTPUT_DATA_DIR, 'model2_sequences_labels.pkl')}\")\n",
    "\n",
    "print(\"\\nCélula 4 concluída. Os datasets para cada modelo foram preparados e salvos.\")\n",
    "print(\"Agora você pode prosseguir para os notebooks de treinamento de cada modelo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "667d1735-36f6-4a2c-814b-a0a5686644f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Resumo da Preparação e Exploração de Dados ---\n",
      "Este notebook concluiu as seguintes etapas:\n",
      "1. Configuração do ambiente e importação das bibliotecas necessárias.\n",
      "2. Carregamento e exploração inicial dos dados de Ground Truth (KSS e PVT-RT).\n",
      "3. Processamento de todos os vídeos do dataset DROZY, incluindo:\n",
      "   - Detecção facial utilizando dlib.\n",
      "   - Extração de 68 landmarks faciais por frame.\n",
      "   - Cálculo de métricas de fadiga (EAR e MAR) por frame.\n",
      "   - Total de 497609 frames processados e salvos em 'processed_data/all_video_features.pkl'.\n",
      "4. Transformação dos dados de nível de frame em sequências temporais e características agregadas por janela:\n",
      "   - Geradas 11006 sequências de 3 segundos (90 frames) para o Modelo 2 (TCN).\n",
      "   - Geradas 11006 amostras com características agregadas por janela para o Modelo 1.\n",
      "   - Dados salvos em 'processed_data/model1_features_labels.pkl' e 'processed_data/model2_sequences_labels.pkl'.\n",
      "\n",
      "Os dados estão agora prontos e pré-processados para o treinamento de ambos os modelos de detecção de fadiga.\n",
      "\n",
      "--- Próximos Passos Sugeridos ---\n",
      "1. Abra o notebook `01_Model_1_Feature_Based_Drowsiness_Detection.ipynb`:\n",
      "   - Carregue os dados de `model1_features_labels.pkl`.\n",
      "   - Treine, avalie e salve o Modelo 1 (SVM/Random Forest/MLP).\n",
      "2. Abra o notebook `02_Model_2_TCN_Temporal_Drowsiness_Detection.ipynb`:\n",
      "   - Carregue os dados de `model2_sequences_labels.pkl`.\n",
      "   - Treine, avalie e salve o Modelo 2 (TCN).\n",
      "3. Finalmente, abra o notebook `03_Ensemble_Voting_System.ipynb`:\n",
      "   - Carregue os modelos treinados 1 e 2.\n",
      "   - Implemente e avalie o sistema de votação ponderada para a decisão final de fadiga.\n",
      "\n",
      "Fim do Notebook de Preparação de Dados.\n"
     ]
    }
   ],
   "source": [
    "# Célula 5: Resumo e Próximos Passos\n",
    "\n",
    "print(\"--- Resumo da Preparação e Exploração de Dados ---\")\n",
    "print(\"Este notebook concluiu as seguintes etapas:\")\n",
    "print(\"1. Configuração do ambiente e importação das bibliotecas necessárias.\")\n",
    "print(\"2. Carregamento e exploração inicial dos dados de Ground Truth (KSS e PVT-RT).\")\n",
    "print(\"3. Processamento de todos os vídeos do dataset DROZY, incluindo:\")\n",
    "print(\"   - Detecção facial utilizando dlib.\")\n",
    "print(\"   - Extração de 68 landmarks faciais por frame.\")\n",
    "print(\"   - Cálculo de métricas de fadiga (EAR e MAR) por frame.\")\n",
    "print(f\"   - Total de {len(df_features)} frames processados e salvos em '{os.path.join(OUTPUT_DATA_DIR, 'all_video_features.pkl')}'.\")\n",
    "print(\"4. Transformação dos dados de nível de frame em sequências temporais e características agregadas por janela:\")\n",
    "print(f\"   - Geradas {len(X_sequences)} sequências de {SEQUENCE_LENGTH_SECONDS} segundos ({SEQUENCE_LENGTH_FRAMES} frames) para o Modelo 2 (TCN).\")\n",
    "print(f\"   - Geradas {len(X_model1_features)} amostras com características agregadas por janela para o Modelo 1.\")\n",
    "print(f\"   - Dados salvos em '{os.path.join(OUTPUT_DATA_DIR, 'model1_features_labels.pkl')}' e '{os.path.join(OUTPUT_DATA_DIR, 'model2_sequences_labels.pkl')}'.\")\n",
    "\n",
    "print(\"\\nOs dados estão agora prontos e pré-processados para o treinamento de ambos os modelos de detecção de fadiga.\")\n",
    "\n",
    "print(\"\\n--- Próximos Passos Sugeridos ---\")\n",
    "print(\"1. Abra o notebook `01_Model_1_Feature_Based_Drowsiness_Detection.ipynb`:\")\n",
    "print(\"   - Carregue os dados de `model1_features_labels.pkl`.\")\n",
    "print(\"   - Treine, avalie e salve o Modelo 1 (SVM/Random Forest/MLP).\")\n",
    "print(\"2. Abra o notebook `02_Model_2_TCN_Temporal_Drowsiness_Detection.ipynb`:\")\n",
    "print(\"   - Carregue os dados de `model2_sequences_labels.pkl`.\")\n",
    "print(\"   - Treine, avalie e salve o Modelo 2 (TCN).\")\n",
    "print(\"3. Finalmente, abra o notebook `03_Ensemble_Voting_System.ipynb`:\")\n",
    "print(\"   - Carregue os modelos treinados 1 e 2.\")\n",
    "print(\"   - Implemente e avalie o sistema de votação ponderada para a decisão final de fadiga.\")\n",
    "\n",
    "print(\"\\nFim do Notebook de Preparação de Dados.\")\n",
    "\n",
    "# Opcional: Limpeza de variáveis para liberar memória, se necessário, em ambientes com recursos limitados\n",
    "# del df_features, all_extracted_features, all_sequences, all_labels, X_sequences, y_labels_tcn, X_model1_features, y_labels_model1\n",
    "# import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c14d6-7d05-411a-8bb7-18b22a00eac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Teste",
   "language": "python",
   "name": "venv_test_proj2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
