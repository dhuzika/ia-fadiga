{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7727da86-b07d-4a56-9ff4-898ad0f67277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 23:28:02.381484: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-22 23:28:02.389975: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747967282.399731  474636 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747967282.402532  474636 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747967282.410386  474636 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747967282.410401  474636 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747967282.410402  474636 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747967282.410403  474636 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-22 23:28:02.413094: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Célula 1: Configuração e Importações\n",
    "\n",
    "# Importação de bibliotecas essenciais\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Importações para TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from tensorflow.keras.metrics import Precision, Recall, F1Score # Para métricas personalizadas se necessário\n",
    "\n",
    "# Importações para Machine Learning (avaliação)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # Pode ser útil se as features não foram padronizadas antes\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "from joblib import dump, load # Para salvar e carregar dados e modelos\n",
    "\n",
    "# Para a camada TCN, usaremos uma implementação comum.\n",
    "# Se você não tiver a biblioteca 'keras-tcn' instalada, ela precisará ser instalada:\n",
    "# !pip install keras-tcn \n",
    "# (Nota: A TCN será implementada manualmente abaixo, então a biblioteca keras-tcn não é estritamente necessária\n",
    "# para esta versão do código, mas a importação está mantida caso você queira alternar implementações)\n",
    "try:\n",
    "    from tcn import TCN #\n",
    "    from tensorflow.keras.utils import get_custom_objects #\n",
    "    # get_custom_objects().update({'TCN': TCN}) # Versões antigas\n",
    "except ImportError:\n",
    "    print(\"Biblioteca 'keras-tcn' não encontrada. A implementação manual da TCN será usada.\") #\n",
    "\n",
    "# Configurações de exibição do Pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Desativar avisos\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso.\")\n",
    "\n",
    "# Definir caminho para os dados pré-processados\n",
    "DATA_DIR = 'processed_data' \n",
    "\n",
    "# Nome do arquivo onde as sequências e rótulos foram salvos\n",
    "SEQUENCES_FILE = 'model2_sequences_labels.pkl' #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99e1823b-e336-4e78-810b-98b7a1859b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dados de sequências pré-processados...\n",
      "Dados carregados.\n",
      "Formato das sequências (X): (11006, 90, 2)\n",
      "Formato dos rótulos (y): (11006,)\n",
      "Distribuição dos rótulos de fadiga: {0: 6889, 1: 4117}\n",
      "Ocorreu um erro ao carregar ou processar os dados: 1 is not in list\n",
      "\n",
      "Dividindo o dataset em conjuntos de treino, validação e teste...\n",
      "Tamanho do conjunto de Treino: X=(7043, 90, 2), y=(7043,)\n",
      "Tamanho do conjunto de Validação: X=(1761, 90, 2), y=(1761,)\n",
      "Tamanho do conjunto de Teste: X=(2202, 90, 2), y=(2202,)\n",
      "\n",
      "Padronizando as características (StandardScaler)...\n",
      "Padronização concluída.\n",
      "Dados prontos para o treinamento do Modelo (CNN+TCN).\n"
     ]
    }
   ],
   "source": [
    "# Célula 2: Carregamento, Pré-processamento e Divisão dos Dados\n",
    "\n",
    "print(\"Carregando dados de sequências pré-processados...\") #\n",
    "\n",
    "try:\n",
    "    # Carregar o dicionário contendo 'sequences' e 'labels'\n",
    "    loaded_data = load(os.path.join(DATA_DIR, SEQUENCES_FILE))\n",
    "    \n",
    "    X = loaded_data['sequences'] # As sequências de características (num_samples, timesteps, num_features)\n",
    "    y = loaded_data['labels']   # Os rótulos de fadiga (0 ou 1) para cada sequência\n",
    "\n",
    "    print(f\"Dados carregados.\")\n",
    "    print(f\"Formato das sequências (X): {X.shape}\") #\n",
    "    print(f\"Formato dos rótulos (y): {y.shape}\") #\n",
    "    \n",
    "    unique_labels, counts = np.unique(y, return_counts=True)\n",
    "    label_distribution = dict(zip(unique_labels, counts))\n",
    "    print(f\"Distribuição dos rótulos de fadiga: {label_distribution}\") #\n",
    "    if 1 in unique_labels:\n",
    "        print(f\"Porcentagem da classe 'Fadigado' (1): {counts[counts.tolist().index(1)] / np.sum(counts):.2%}\") #\n",
    "    else:\n",
    "        print(\"Classe 'Fadigado' (1) não encontrada.\") #\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivo '{SEQUENCES_FILE}' não encontrado em '{DATA_DIR}'.\") #\n",
    "    print(\"Por favor, certifique-se de que o notebook '00_Data_Preparation_and_Exploration.ipynb' foi executado corretamente.\") #\n",
    "    # exit()\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao carregar ou processar os dados: {e}\")\n",
    "    # exit()\n",
    "\n",
    "print(\"\\nDividindo o dataset em conjuntos de treino, validação e teste...\")\n",
    "\n",
    "# Divisão estratificada para manter a proporção das classes\n",
    "# 1. Divisão inicial: 80% treino + validação, 20% teste\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 2. Divisão do conjunto de treino+validação: 80% treino, 20% validação\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2, random_state=42, stratify=y_train_val\n",
    ")\n",
    "\n",
    "print(f\"Tamanho do conjunto de Treino: X={X_train.shape}, y={y_train.shape}\") #\n",
    "print(f\"Tamanho do conjunto de Validação: X={X_val.shape}, y={y_val.shape}\") #\n",
    "print(f\"Tamanho do conjunto de Teste: X={X_test.shape}, y={y_test.shape}\") #\n",
    "\n",
    "# Padronização das características\n",
    "# X tem shape (num_samples, timesteps, num_features)\n",
    "n_samples, n_timesteps, n_features = X_train.shape #\n",
    "\n",
    "# Achatar para (num_samples * timesteps, num_features)\n",
    "X_train_reshaped = X_train.reshape(-1, n_features)\n",
    "X_val_reshaped = X_val.reshape(-1, n_features)\n",
    "X_test_reshaped = X_test.reshape(-1, n_features)\n",
    "\n",
    "print(\"\\nPadronizando as características (StandardScaler)...\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled_reshaped = scaler.fit_transform(X_train_reshaped)\n",
    "X_val_scaled_reshaped = scaler.transform(X_val_reshaped) #\n",
    "X_test_scaled_reshaped = scaler.transform(X_test_reshaped)\n",
    "\n",
    "# Reformatar de volta para (num_samples, timesteps, num_features)\n",
    "X_train_scaled = X_train_scaled_reshaped.reshape(n_samples, n_timesteps, n_features)\n",
    "X_val_scaled = X_val_scaled_reshaped.reshape(X_val.shape[0], n_timesteps, n_features)\n",
    "X_test_scaled = X_test_scaled_reshaped.reshape(X_test.shape[0], n_timesteps, n_features)\n",
    "\n",
    "print(\"Padronização concluída.\")\n",
    "print(\"Dados prontos para o treinamento do Modelo (CNN+TCN).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d13ddb-f9e7-4d6c-bbb8-3603bc9d5992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Implementando e Treinando o Modelo (CNN + TCN Manual Robusta) ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# --- Definindo a arquitetura completa da CNN+TCN ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m input_shape_dims = (\u001b[43mX_train_scaled\u001b[49m.shape[\u001b[32m1\u001b[39m], X_train_scaled.shape[\u001b[32m2\u001b[39m])\n\u001b[32m     71\u001b[39m inputs = keras.Input(shape=input_shape_dims)\n\u001b[32m     72\u001b[39m x = inputs\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# Célula 3: Implementação e Treinamento da CNN+TCN\n",
    "\n",
    "print(\"--- Implementando e Treinando o Modelo (CNN + TCN Manual Robusta) ---\")\n",
    "\n",
    "# --- Parâmetros da CNN ---\n",
    "NUM_CNN_FILTERS = 64  # Número de filtros para as camadas CNN\n",
    "CNN_KERNEL_SIZE = 3   # Tamanho do kernel para as camadas CNN\n",
    "CNN_DROPOUT_RATE = 0.25 # Taxa de Dropout para camadas CNN (geralmente menor que TCN)\n",
    "\n",
    "# --- Parâmetros da TCN ---\n",
    "NUM_TCN_BLOCKS = 3     # Número de blocos residuais TCN (ajustado, pode ser 2-4)\n",
    "NUM_FILTERS_TCN = 64   # Número de filtros convolucionais em cada camada TCN\n",
    "KERNEL_SIZE_TCN = 2    # Tamanho do kernel para as convoluções TCN\n",
    "TCN_DROPOUT_RATE = 0.5 # Taxa de Dropout para TCN\n",
    "LEARNING_RATE = 0.001  # Taxa de aprendizado do otimizador\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# --- Definição do Bloco Residual TCN (Manual Robusto) ---\n",
    "def residual_block_manual(x, nb_filters, kernel_size, dilation_rate, dropout_rate, name='tcn_block'):\n",
    "    \"\"\"\n",
    "    Constrói um bloco residual da TCN utilizando apenas camadas Keras nativas.\n",
    "    Incorpora duas convoluções dilatadas, Batch Normalization, ativação ReLU, Dropout\n",
    "    e uma conexão residual (skip connection), com regularização L2.\n",
    "    \"\"\"\n",
    "    input_to_block = x\n",
    "\n",
    "    # Bloco convolucional 1 com regularização L2\n",
    "    conv1 = layers.Conv1D(\n",
    "        filters=nb_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        dilation_rate=dilation_rate,\n",
    "        padding='causal',\n",
    "        kernel_regularizer=regularizers.l2(0.0005), #\n",
    "        name=f'{name}_conv1_d{dilation_rate}'\n",
    "    )(x)\n",
    "    conv1 = layers.BatchNormalization(name=f'{name}_bn1_d{dilation_rate}')(conv1)\n",
    "    conv1 = layers.Activation('relu', name=f'{name}_relu1_d{dilation_rate}')(conv1)\n",
    "    conv1 = layers.Dropout(dropout_rate, name=f'{name}_dropout1_d{dilation_rate}')(conv1)\n",
    "\n",
    "    # Bloco convolucional 2 com regularização L2\n",
    "    conv2 = layers.Conv1D(\n",
    "        filters=nb_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        dilation_rate=dilation_rate,\n",
    "        padding='causal',\n",
    "        kernel_regularizer=regularizers.l2(0.0005), #\n",
    "        name=f'{name}_conv2_d{dilation_rate}'\n",
    "    )(conv1)\n",
    "    conv2 = layers.BatchNormalization(name=f'{name}_bn2_d{dilation_rate}')(conv2)\n",
    "    conv2 = layers.Activation('relu', name=f'{name}_relu2_d{dilation_rate}')(conv2)\n",
    "    conv2 = layers.Dropout(dropout_rate, name=f'{name}_dropout2_d{dilation_rate}')(conv2)\n",
    "\n",
    "    # Conexão residual\n",
    "    if input_to_block.shape[-1] != nb_filters:\n",
    "        input_to_block = layers.Conv1D(\n",
    "            filters=nb_filters,\n",
    "            kernel_size=1,\n",
    "            padding='same',\n",
    "            kernel_regularizer=regularizers.l2(0.0005), #\n",
    "            name=f'{name}_skip_conv_d{dilation_rate}'\n",
    "        )(input_to_block)\n",
    "    \n",
    "    output = layers.add([input_to_block, conv2], name=f'{name}_add_d{dilation_rate}') #\n",
    "    output = layers.Activation('relu', name=f'{name}_final_relu_d{dilation_rate}')(output) #\n",
    "    return output\n",
    "\n",
    "\n",
    "# --- Definindo a arquitetura completa da CNN+TCN ---\n",
    "input_shape_dims = (X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "inputs = keras.Input(shape=input_shape_dims)\n",
    "x = inputs\n",
    "\n",
    "# --- Camadas CNN ---\n",
    "# Bloco CNN 1\n",
    "x = layers.Conv1D(filters=NUM_CNN_FILTERS, kernel_size=CNN_KERNEL_SIZE, padding='same', \n",
    "                  kernel_regularizer=regularizers.l2(0.0005), name='cnn_conv1')(x)\n",
    "x = layers.BatchNormalization(name='cnn_bn1')(x)\n",
    "x = layers.Activation('relu', name='cnn_relu1')(x)\n",
    "x = layers.Dropout(CNN_DROPOUT_RATE, name='cnn_dropout1')(x)\n",
    "# Opcional: MaxPooling1D. Se usar, cuidado com o encurtamento da sequência.\n",
    "# x = layers.MaxPooling1D(pool_size=2, name='cnn_pool1')(x) \n",
    "\n",
    "# (Opcional) Bloco CNN 2\n",
    "# x = layers.Conv1D(filters=NUM_CNN_FILTERS, kernel_size=CNN_KERNEL_SIZE, padding='same', \n",
    "#                   kernel_regularizer=regularizers.l2(0.0005), name='cnn_conv2')(x)\n",
    "# x = layers.BatchNormalization(name='cnn_bn2')(x)\n",
    "# x = layers.Activation('relu', name='cnn_relu2')(x)\n",
    "# x = layers.Dropout(CNN_DROPOUT_RATE, name='cnn_dropout2')(x)\n",
    "\n",
    "# --- Camadas TCN ---\n",
    "# A saída 'x' das camadas CNN agora alimenta as camadas TCN\n",
    "for i in range(NUM_TCN_BLOCKS):\n",
    "    dilation_rate = 2 ** i\n",
    "    x = residual_block_manual(x, NUM_FILTERS_TCN, KERNEL_SIZE_TCN, dilation_rate, TCN_DROPOUT_RATE, name=f'tcn_block_{i}')\n",
    "\n",
    "# --- Camada de Saída ---\n",
    "x = layers.GlobalAveragePooling1D(name='global_avg_pooling')(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid', name='output_layer')(x)\n",
    "\n",
    "model_cnn_tcn = keras.Model(inputs=inputs, outputs=outputs, name='CNN_TCN_Manual_Model')\n",
    "\n",
    "# --- Compilação do Modelo ---\n",
    "model_cnn_tcn.compile(optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "model_cnn_tcn.summary()\n",
    "\n",
    "# --- Callbacks para Treinamento ---\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True) #\n",
    "\n",
    "MODEL_SAVE_PATH_CNN_TCN = os.path.join(DATA_DIR, 'best_model_cnn_tcn.keras') # Nome do arquivo atualizado\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    MODEL_SAVE_PATH_CNN_TCN,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Cálculo de pesos para lidar com desbalanceamento de classes\n",
    "class_weights = {}\n",
    "unique_labels_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "total_samples_train = len(y_train)\n",
    "\n",
    "if len(counts_train) == 2: # Garante que ambas as classes estão presentes\n",
    "    class_weights[0] = total_samples_train / (2.0 * counts_train[0])\n",
    "    class_weights[1] = total_samples_train / (2.0 * counts_train[1])\n",
    "    print(f\"\\nPesos de Classe Calculados: {class_weights}\")\n",
    "else:\n",
    "    print(\"\\nNão foi possível calcular os pesos de classe (uma classe pode estar faltando no y_train). Usando pesos None.\")\n",
    "    class_weights = None\n",
    "\n",
    "\n",
    "print(\"\\nIniciando treinamento do Modelo (CNN+TCN Manual Robusta)...\")\n",
    "\n",
    "# --- Treinamento do Modelo ---\n",
    "history = model_cnn_tcn.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=200, # Pode precisar ajustar\n",
    "    batch_size=32, #\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTreinamento da CNN+TCN Manual Robusta concluído.\")\n",
    "\n",
    "# --- Plotar histórico de treinamento ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Acurácia de Treino')\n",
    "plt.plot(history.history['val_accuracy'], label='Acurácia de Validação')\n",
    "plt.title('Acurácia do Modelo CNN+TCN ao longo das Épocas')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Perda de Treino')\n",
    "plt.plot(history.history['val_loss'], label='Perda de Validação')\n",
    "plt.title('Perda do Modelo CNN+TCN ao longo das Épocas')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Carregar os melhores pesos salvos pelo ModelCheckpoint\n",
    "try:\n",
    "    # Nota: Keras salva o modelo inteiro (arquitetura + pesos) no formato .keras\n",
    "    # Não é necessário registrar custom objects para a implementação manual.\n",
    "    model_cnn_tcn = keras.models.load_model(MODEL_SAVE_PATH_CNN_TCN)\n",
    "    print(f\"Melhor modelo CNN+TCN carregado de: {MODEL_SAVE_PATH_CNN_TCN}\")\n",
    "except Exception as e:\n",
    "    print(f\"Não foi possível carregar o melhor modelo CNN+TCN: {e}. Usando o modelo final do treinamento.\") #\n",
    "\n",
    "print(\"\\nTreinamento do Modelo CNN+TCN finalizado. Próximo passo: Avaliação do Modelo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6f5a304-949a-4349-a823-604081a76bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Avaliando o Modelo (CNN+TCN) no Conjunto de Teste ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_cnn_tcn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Avaliando o Modelo (CNN+TCN) no Conjunto de Teste ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# model_cnn_tcn deve conter o modelo com os melhores pesos de validação, carregado no final da Célula 3.\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 1. Avaliar o modelo no conjunto de teste [cite: 43]\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m loss_test, accuracy_test = \u001b[43mmodel_cnn_tcn\u001b[49m.evaluate(X_test_scaled, y_test, verbose=\u001b[32m0\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mResultados no Conjunto de Teste:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Perda (Loss): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_test\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model_cnn_tcn' is not defined"
     ]
    }
   ],
   "source": [
    "# Célula 4: Avaliação do Modelo no Conjunto de Teste\n",
    "\n",
    "print(\"--- Avaliando o Modelo (CNN+TCN) no Conjunto de Teste ---\")\n",
    "\n",
    "# model_cnn_tcn deve conter o modelo com os melhores pesos de validação, carregado no final da Célula 3.\n",
    "\n",
    "# 1. Avaliar o modelo no conjunto de teste [cite: 43]\n",
    "loss_test, accuracy_test = model_cnn_tcn.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"\\nResultados no Conjunto de Teste:\")\n",
    "print(f\"  Perda (Loss): {loss_test:.4f}\")\n",
    "print(f\"  Acurácia (Accuracy): {accuracy_test:.4f}\")\n",
    "\n",
    "# 2. Fazer previsões no conjunto de teste\n",
    "y_pred_proba = model_cnn_tcn.predict(X_test_scaled).ravel() # Probabilidades\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)             # Classes (0 ou 1)\n",
    "\n",
    "# 3. Calcular Métricas de Classificação Detalhadas\n",
    "print(\"\\nMétricas de Classificação Detalhadas:\")\n",
    "print(f\"  Acurácia Geral: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  Precisão (Precision): {precision_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "print(f\"  Recall (Sensibilidade): {recall_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "print(f\"  F1-Score: {f1_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "\n",
    "# 4. Matriz de Confusão\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nMatriz de Confusão:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualizar a Matriz de Confusão\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Não Fadigado (0)', 'Fadigado (1)'], # [cite: 44]\n",
    "            yticklabels=['Não Fadigado (0)', 'Fadigado (1)']) # [cite: 44]\n",
    "plt.xlabel('Previsão')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusão do Modelo TCN')\n",
    "plt.show()\n",
    "\n",
    "# 5. Curva ROC e AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Classificador Aleatório')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de Falsos Positivos (FPR)')\n",
    "plt.ylabel('Taxa de Verdadeiros Positivos (TPR)')\n",
    "plt.title('Curva ROC do Modelo CNN+TCN')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAvaliação do Modelo TCN concluída.\") # [cite: 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "751b48c3-96c8-4f1d-93cf-75face63283d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Salvando o Modelo CNN+TCN e os Pré-processadores ---\n",
      "O melhor modelo CNN+TCN já foi salvo em: processed_data/best_model_cnn_tcn.keras\n",
      "StandardScaler salvo em: processed_data/scaler_model_cnn_tcn.joblib\n",
      "Seus rótulos (y) já eram numéricos (0/1). Não é necessário salvar um LabelEncoder.\n",
      "\n",
      "Salvamento do modelo CNN+TCN e pré-processadores concluído. O modelo está pronto para inferência.\n"
     ]
    }
   ],
   "source": [
    "# Célula 5: Salvar o Modelo Final e os Pré-processadores\n",
    "\n",
    "print(\"--- Salvando o Modelo CNN+TCN e os Pré-processadores ---\")\n",
    "\n",
    "# 1. Salvar o Modelo CNN+TCN\n",
    "# MODEL_SAVE_PATH_CNN_TCN já está definido na Célula 3 e o melhor modelo já foi salvo.\n",
    "# A linha no final da Célula 3 já deveria ter carregado essa melhor versão em 'model_cnn_tcn'.\n",
    "print(f\"O melhor modelo CNN+TCN já foi salvo em: {MODEL_SAVE_PATH_CNN_TCN}\") # [cite: 46]\n",
    "\n",
    "# Se você quiser salvar o modelo 'model_cnn_tcn' novamente (o estado atual após o treinamento\n",
    "# e potencial restauração pelo EarlyStopping), pode fazer assim:\n",
    "# model_cnn_tcn.save(MODEL_SAVE_PATH_CNN_TCN)\n",
    "# print(f\"Modelo CNN+TCN salvo novamente em: {MODEL_SAVE_PATH_CNN_TCN}\")\n",
    "\n",
    "\n",
    "# 2. Salvar o StandardScaler usado para as features\n",
    "SCALER_SAVE_PATH = os.path.join(DATA_DIR, 'scaler_model_cnn_tcn.joblib') # Nome do arquivo atualizado\n",
    "\n",
    "try:\n",
    "    dump(scaler, SCALER_SAVE_PATH)\n",
    "    print(f\"StandardScaler salvo em: {SCALER_SAVE_PATH}\")\n",
    "except NameError:\n",
    "    print(\"Erro: 'scaler' não está definido. Certifique-se de que a Célula 2 foi executada corretamente.\") # [cite: 47]\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao salvar o StandardScaler: {e}\") # [cite: 47]\n",
    "\n",
    "\n",
    "# 3. Salvar o LabelEncoder (se usado)\n",
    "# Neste caso, os rótulos são 0 e 1, então não é estritamente necessário. [cite: 48]\n",
    "print(\"Seus rótulos (y) já eram numéricos (0/1). Não é necessário salvar um LabelEncoder.\") # [cite: 49]\n",
    "\n",
    "print(\"\\nSalvamento do modelo CNN+TCN e pré-processadores concluído. O modelo está pronto para inferência.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f5ec6a-8b28-48a6-996c-e8c818b2f8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 12:56:58.217378: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-26 12:56:58.225372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748275018.235963   33248 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748275018.238538   33248 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748275018.245357   33248 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748275018.245384   33248 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748275018.245385   33248 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748275018.245386   33248 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-26 12:56:58.248757: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testando o Modelo CNN-TCN com Câmera ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748275019.707041   33248 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1166 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo CNN-TCN carregado de: processed_data/best_model_cnn_tcn.keras\n",
      "StandardScaler carregado de: processed_data/scaler_model_cnn_tcn.joblib\n",
      "Configuração do modelo: TIMESTEPS=90, NUM_FEATURES_MODEL=2\n",
      "\n",
      "Iniciando captura da câmera. Pressione 'q' para sair.\n",
      "O modelo TCN espera 90 amostras de tempo (frames) com 2 características cada.\n",
      "As características extraídas serão: [EAR médio, MAR].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748275020.248924   33248 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1748275020.280716   33452 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 575.51.03), renderer: NVIDIA GeForce RTX 4060 Ti/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1748275020.283053   33435 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748275020.290310   33433 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748275020.708543   33433 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748275026.809869   33395 service.cc:152] XLA service 0x7fbbb8002480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1748275026.809884   33395 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9\n",
      "2025-05-26 12:57:06.819940: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1748275026.874562   33395 cuda_dnn.cc:529] Loaded cuDNN version 90501\n",
      "I0000 00:00:1748275027.232632   33395 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste finalizado.\n"
     ]
    }
   ],
   "source": [
    "# Célula de Teste do Modelo CNN-TCN com Câmera em Tempo Real (usando EAR e MAR)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow import keras\n",
    "from joblib import load\n",
    "import os\n",
    "from collections import deque\n",
    "import math # Para a função de distância\n",
    "\n",
    "print(\"--- Testando o Modelo CNN-TCN com Câmera ---\")\n",
    "\n",
    "# --- 0. Constantes e Caminhos (ajuste conforme necessário) ---\n",
    "DATA_DIR = 'processed_data' # Deve ser o mesmo diretório onde você salvou o modelo e o scaler\n",
    "MODEL_FILE = 'best_model_cnn_tcn.keras' # Nome do arquivo do seu modelo TCN treinado\n",
    "SCALER_FILE = 'scaler_model_cnn_tcn.joblib' # Nome do arquivo do seu scaler para o modelo TCN\n",
    "\n",
    "MODEL_PATH = os.path.join(DATA_DIR, MODEL_FILE)\n",
    "SCALER_PATH = os.path.join(DATA_DIR, SCALER_FILE)\n",
    "\n",
    "# Tentar carregar o modelo e o scaler\n",
    "try:\n",
    "    # A variável agora se chama 'model_cnn_tcn' para clareza\n",
    "    model_cnn_tcn = keras.models.load_model(MODEL_PATH)\n",
    "    print(f\"Modelo CNN-TCN carregado de: {MODEL_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar o modelo: {e}\")\n",
    "    print(\"Certifique-se de que o caminho e o nome do arquivo do modelo TCN estão corretos.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    scaler = load(SCALER_PATH)\n",
    "    print(f\"StandardScaler carregado de: {SCALER_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar o scaler: {e}\")\n",
    "    print(\"Certifique-se de que o caminho e o nome do arquivo do scaler TCN estão corretos.\")\n",
    "    exit()\n",
    "\n",
    "# --- 1. Obter Parâmetros do Modelo Carregado ---\n",
    "try:\n",
    "    # A forma da entrada do modelo deve ser (None, TIMESTEPS, NUM_FEATURES_MODEL)\n",
    "    _, TIMESTEPS, NUM_FEATURES_MODEL = model_cnn_tcn.input_shape\n",
    "    print(f\"Configuração do modelo: TIMESTEPS={TIMESTEPS}, NUM_FEATURES_MODEL={NUM_FEATURES_MODEL}\")\n",
    "    if NUM_FEATURES_MODEL != 2:\n",
    "        print(f\"AVISO CRÍTICO: O modelo TCN espera {NUM_FEATURES_MODEL} características, mas a lógica de extração está configurada para 2 (EAR e MAR).\")\n",
    "        print(\"VOCÊ PRECISA AJUSTAR A FUNÇÃO 'extract_features_from_face_metrics' se seu modelo TCN não foi treinado com EAR e MAR, ou se o número de features for diferente de 2.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao obter input_shape do modelo: {e}\")\n",
    "    print(\"Não foi possível determinar TIMESTEPS e NUM_FEATURES_MODEL a partir do modelo.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Configuração do MediaPipe Face Mesh ---\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True, # Obtém landmarks mais detalhados para íris, etc.\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# --- 3. Funções Auxiliares para Cálculo de Métricas (EAR e MAR) ---\n",
    "# Copiadas do seu código LSTM, pois são as features que queremos para o TCN também.\n",
    "\n",
    "def calculate_distance(point1, point2):\n",
    "    \"\"\"Calcula a distância euclidiana entre dois pontos 3D (landmarks).\"\"\"\n",
    "    return math.sqrt((point1.x - point2.x)**2 + (point1.y - point2.y)**2 + (point1.z - point2.z)**2)\n",
    "\n",
    "def calculate_ear(eye_landmarks, all_landmarks):\n",
    "    \"\"\"\n",
    "    Calcula o Eye Aspect Ratio (EAR) para um olho.\n",
    "    A fórmula é (dist(P2,P6) + dist(P3,P5)) / (2 * dist(P1,P4)).\n",
    "    Os índices dos landmarks são baseados na documentação do MediaPipe Face Mesh.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Pontos verticais\n",
    "        p2 = all_landmarks[eye_landmarks[0]]\n",
    "        p6 = all_landmarks[eye_landmarks[1]]\n",
    "        p3 = all_landmarks[eye_landmarks[2]]\n",
    "        p5 = all_landmarks[eye_landmarks[3]]\n",
    "        # Pontos horizontais\n",
    "        p1 = all_landmarks[eye_landmarks[4]]\n",
    "        p4 = all_landmarks[eye_landmarks[5]]\n",
    "\n",
    "        vertical_dist1 = calculate_distance(p2, p6)\n",
    "        vertical_dist2 = calculate_distance(p3, p5)\n",
    "        horizontal_dist = calculate_distance(p1, p4)\n",
    "\n",
    "        if horizontal_dist == 0:\n",
    "            return 0.0\n",
    "        ear = (vertical_dist1 + vertical_dist2) / (2.0 * horizontal_dist)\n",
    "        return ear\n",
    "    except IndexError:\n",
    "        print(\"Erro de índice ao acessar landmarks do olho para EAR. Retornando 0.0.\")\n",
    "        return 0.0\n",
    "\n",
    "def calculate_mar(mouth_landmarks, all_landmarks):\n",
    "    \"\"\"\n",
    "    Calcula o Mouth Aspect Ratio (MAR).\n",
    "    A fórmula é dist(P_topo, P_base) / dist(P_esquerda, P_direita).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        p_topo = all_landmarks[mouth_landmarks[0]]\n",
    "        p_base = all_landmarks[mouth_landmarks[1]]\n",
    "        p_esquerda = all_landmarks[mouth_landmarks[2]]\n",
    "        p_direita = all_landmarks[mouth_landmarks[3]]\n",
    "\n",
    "        vertical_dist = calculate_distance(p_topo, p_base)\n",
    "        horizontal_dist = calculate_distance(p_esquerda, p_direita)\n",
    "\n",
    "        if horizontal_dist == 0:\n",
    "            return 0.0\n",
    "        mar = vertical_dist / horizontal_dist\n",
    "        return mar\n",
    "    except IndexError:\n",
    "        print(\"Erro de índice ao acessar landmarks da boca para MAR. Retornando 0.0.\")\n",
    "        return 0.0\n",
    "\n",
    "# Índices dos landmarks do MediaPipe para olhos e boca\n",
    "RIGHT_EYE_LANDMARK_INDICES = [160, 144, 158, 153, 33, 133] # P2,P6,P3,P5,P1,P4\n",
    "LEFT_EYE_LANDMARK_INDICES = [387, 373, 385, 380, 263, 362] # P2,P6,P3,P5,P1,P4\n",
    "MOUTH_LANDMARK_INDICES = [13, 14, 61, 291] # Topo, Base, Esquerda, Direita (para MAR)\n",
    "\n",
    "\n",
    "def extract_features_from_face_metrics(face_landmarks_object_list):\n",
    "    \"\"\"\n",
    "    Extrai EAR e MAR a partir dos landmarks faciais.\n",
    "    Retorna um array NumPy com [ear_medio, mar].\n",
    "    Esta função espera NUM_FEATURES_MODEL = 2.\n",
    "    \"\"\"\n",
    "    features = np.zeros(NUM_FEATURES_MODEL, dtype=np.float32)\n",
    "\n",
    "    if face_landmarks_object_list: # Lista de objetos de landmarks faciais detectados\n",
    "        # Assumindo que estamos processando apenas o primeiro rosto detectado\n",
    "        all_landmarks = face_landmarks_object_list[0].landmark\n",
    "\n",
    "        # Calcular EAR para ambos os olhos e tirar a média\n",
    "        ear_right = calculate_ear(RIGHT_EYE_LANDMARK_INDICES, all_landmarks)\n",
    "        ear_left = calculate_ear(LEFT_EYE_LANDMARK_INDICES, all_landmarks)\n",
    "        average_ear = (ear_right + ear_left) / 2.0\n",
    "\n",
    "        # Calcular MAR\n",
    "        mar = calculate_mar(MOUTH_LANDMARK_INDICES, all_landmarks)\n",
    "\n",
    "        if NUM_FEATURES_MODEL == 2:\n",
    "            features[0] = average_ear\n",
    "            features[1] = mar\n",
    "        else:\n",
    "            print(f\"Aviso: 'extract_features_from_face_metrics' gera EAR e MAR, mas 'NUM_FEATURES_MODEL' é {NUM_FEATURES_MODEL}. Isso pode causar incompatibilidade.\")\n",
    "            if NUM_FEATURES_MODEL > 0: features[0] = average_ear\n",
    "            if NUM_FEATURES_MODEL > 1: features[1] = mar\n",
    "            # As features restantes seriam zero se NUM_FEATURES_MODEL > 2\n",
    "            # ou haveria um erro se NUM_FEATURES_MODEL < 2\n",
    "    return features\n",
    "\n",
    "\n",
    "# --- 4. Loop Principal da Câmera ---\n",
    "cap = cv2.VideoCapture(0) # 0 para a câmera padrão\n",
    "if not cap.isOpened():\n",
    "    print(\"Erro: Não foi possível abrir a câmera. Verifique se ela está conectada e disponível.\")\n",
    "    exit()\n",
    "\n",
    "sequence_buffer = deque(maxlen=TIMESTEPS)\n",
    "prediction_text = \"Aguardando dados...\"\n",
    "prediction_color = (0, 255, 255) # Amarelo\n",
    "\n",
    "print(\"\\nIniciando captura da câmera. Pressione 'q' para sair.\")\n",
    "print(f\"O modelo TCN espera {TIMESTEPS} amostras de tempo (frames) com {NUM_FEATURES_MODEL} características cada.\")\n",
    "if NUM_FEATURES_MODEL == 2:\n",
    "    print(\"As características extraídas serão: [EAR médio, MAR].\")\n",
    "else:\n",
    "    print(f\"AVISO: As características extraídas são EAR e MAR, mas o modelo TCN espera {NUM_FEATURES_MODEL}. Verifique a consistência.\")\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        print(\"Ignorando frame vazio da câmera.\")\n",
    "        continue\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_rgb.flags.writeable = False # Otimização: tornar a imagem não-escrevível antes do processamento\n",
    "    results = face_mesh.process(image_rgb)\n",
    "    image_rgb.flags.writeable = True # Tornar a imagem escrevível novamente para desenhar\n",
    "\n",
    "    image_output = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR) # Converter de volta para BGR para OpenCV\n",
    "\n",
    "    current_features = None\n",
    "    if results.multi_face_landmarks:\n",
    "        # Desenhar landmarks (opcional)\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=image_output,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp.solutions.drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=image_output,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp.solutions.drawing_styles.get_default_face_mesh_contours_style())\n",
    "            \n",
    "        # Extrair as métricas (EAR, MAR)\n",
    "        current_features = extract_features_from_face_metrics(results.multi_face_landmarks)\n",
    "    else:\n",
    "        # Nenhum rosto detectado, preenche com zeros para manter o tamanho da sequência\n",
    "        current_features = np.zeros(NUM_FEATURES_MODEL, dtype=np.float32)\n",
    "        prediction_text = \"Nenhum rosto detectado\"\n",
    "        prediction_color = (0,0,255) # Vermelho\n",
    "\n",
    "    # Adicionar as características atuais ao buffer\n",
    "    if current_features is not None:\n",
    "        sequence_buffer.append(current_features)\n",
    "\n",
    "    # Se o buffer estiver cheio (ou seja, temos TIMESTEPS quadros de dados)\n",
    "    if len(sequence_buffer) == TIMESTEPS:\n",
    "        sequence_np = np.array(list(sequence_buffer)) # Shape: (TIMESTEPS, NUM_FEATURES_MODEL)\n",
    "\n",
    "        # O scaler foi ajustado em dados com shape (num_samples * timesteps, num_features)\n",
    "        # Para transformar uma única sequência, precisamos achatá-la para (timesteps, num_features)\n",
    "        sequence_reshaped_for_scaler = sequence_np.reshape(-1, NUM_FEATURES_MODEL)\n",
    "        \n",
    "        try:\n",
    "            if sequence_reshaped_for_scaler.shape[1] != scaler.n_features_in_:\n",
    "                print(f\"Erro crítico de dimensionalidade antes do scaler: Features extraídas ({sequence_reshaped_for_scaler.shape[1]}) != Features esperadas pelo scaler ({scaler.n_features_in_}).\")\n",
    "                print(\"Verifique NUM_FEATURES_MODEL e a lógica de extract_features_from_face_metrics.\")\n",
    "                prediction_text = \"Erro Dim Scaler!\"\n",
    "                prediction_color = (0,0,255) # Vermelho\n",
    "                sequence_scaled_reshaped = np.zeros_like(sequence_reshaped_for_scaler) # Para evitar crash, mas predição inválida\n",
    "            else:\n",
    "                sequence_scaled_reshaped = scaler.transform(sequence_reshaped_for_scaler)\n",
    "                # Resetar a cor de predição se um erro anterior foi resolvido\n",
    "                if prediction_text in [\"Erro Dim Scaler!\", \"Erro no Scaler!\", \"Nenhum rosto detectado\"] and current_features.any():\n",
    "                    prediction_color = (0, 255, 255) # Amarelo para aguardar nova predição\n",
    "        \n",
    "        except ValueError as ve:\n",
    "            print(f\"Erro ao aplicar o scaler: {ve}\")\n",
    "            print(f\"Scaler esperava {scaler.n_features_in_} features, obteve {sequence_reshaped_for_scaler.shape[1]}.\")\n",
    "            prediction_text = \"Erro no Scaler!\"\n",
    "            prediction_color = (0,0,255) # Vermelho\n",
    "            sequence_scaled_reshaped = np.zeros_like(sequence_reshaped_for_scaler) # Para evitar crash\n",
    "        \n",
    "        # Reformatar para (1, TIMESTEPS, NUM_FEATURES_MODEL) para o modelo TCN\n",
    "        sequence_scaled = sequence_scaled_reshaped.reshape(1, TIMESTEPS, NUM_FEATURES_MODEL)\n",
    "\n",
    "        # Só fazer a predição se não houve erro grave anterior\n",
    "        if prediction_text not in [\"Erro Dim Scaler!\", \"Erro no Scaler!\", \"Nenhum rosto detectado\"]:\n",
    "            prediction_proba = model_cnn_tcn.predict(sequence_scaled, verbose=0)[0] # verbose=0 para não printar progresso\n",
    "            prediction_class = (prediction_proba > 0.5).astype(int)[0]\n",
    "            prediction_text = f\"Classe: {prediction_class} (Prob: {prediction_proba[0]:.2f})\"\n",
    "            # Verde para classe 1 (Fadigado, por exemplo), Azul claro para 0 (Normal)\n",
    "            prediction_color = (0, 255, 0) if prediction_class == 1 else (255, 100, 100)\n",
    "            \n",
    "    # Mostrar a predição na tela\n",
    "    cv2.putText(image_output, prediction_text, (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, prediction_color, 2)\n",
    "    cv2.putText(image_output, f\"Buffer: {len(sequence_buffer)}/{TIMESTEPS}\", (10, 60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 0), 1)\n",
    "\n",
    "    # Mostrar a imagem\n",
    "    cv2.imshow('Teste Modelo CNN-TCN com Camera', image_output)\n",
    "\n",
    "    # Pressione 'q' para sair\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# --- 5. Limpeza ---\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "if 'face_mesh' in locals() and face_mesh is not None:\n",
    "    face_mesh.close()\n",
    "print(\"Teste finalizado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801fc276-a2de-40bb-8ae6-feffc41d6537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a32c86b-01d0-46fc-a4af-7cbf48b06bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Teste",
   "language": "python",
   "name": "venv_test_proj2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
